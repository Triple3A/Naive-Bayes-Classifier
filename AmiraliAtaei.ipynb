{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# پروژه سوم\n",
    "## هدف پروژه:\n",
    "#### در این پروژه باید کامنت‌هایی که نشان دهنده پیشنهاد شدن محصول است را به وسیله داده‌هایی که از قبل داریم تخمین بزنیم."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## سوال ۱:\n",
    "1. روش stemming: این روش نشانه‌های جمع و ضمیر افعال را حذف می‌کند.\n",
    "2. روش lemmatization: این روش ریشه افعال را به جای افعال قرار می‌دهد. اشکالی که این روش دارد این است که تفاوتی بین فعل منفی و مثبت قائل نمی‌شود.\n",
    "#### با توجه به نتیجه بدست آمده, روش lemmatization نتیجه بهتری داشته است. به دلیل اینکه زمان افعال برای آن مهم نیست و باعث می‌شود تعداد افعال یکسان بیشتر شود."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## سوال ۲:\n",
    "### prior: احتمال recommended بودن یا نبودن یک نظر را نشان می‌دهد و با تقسیم تعداد نظرهای recommended به کل نظرها بدست می‌آید.\n",
    "### posterior: احتمال recommended بودن نظر به شرط کلمات آمده را نشان می‌دهد و با فرمول بیز می‌توان با ضرب likelihood ها در prior بدست آورد.\n",
    "### likelihood: احتمال اینکه کلمه‌ای در جایگاه i قرار بگیرد به شرط recommended بودن آن نظر را نشان می‌دهد. احتمال تمام جایگاه‌ها را یکسان درنظر می‌گیریم. بنابراین هر کلمه به شرط اینکه نظر recommended باشد احتمالی دارد که با تقسیم تعداد تکرار آن کلمه در این نوع نظرها تقسیم بر کل کلمات این نوع نظرات محاسبه می‌شود.\n",
    "### evidence: این احتمال نشان دهنده احتمال تشکیل جمله‌های مختلف است. نیازی به محاسبهٔ این احتمال نیست زیرا برای همه ثابت است و می‌توان آنرا درنظر نگرفت."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## سوال ۳:\n",
    "#### اگر کلمه ای در train وجود نداشته باشد, احتمال آن صفر درنظر گرفته می‌شود و با توجه به فرموال بیز, احتمال نهایی صفر می‌شود که دچار خطای زیادی می‌شود زیرا هر دو احتمال صفر می‌شوند یا اگر در یکی از دو نوع نظرات فقط وجود داشته باشد برای دیگری احتمال صفر دارد و قطعا آن یکی نظر را تخمین می‌زند."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## سوال ۴:\n",
    "#### برای هر نظر احتمال likelihood را به این صورت تغییر می‌دهد: صورت همه احتمالات بدست آمده را به علاوه یک می‌کند و مخرج را با تعداد کلمه‌ها (جایگاه‌ها) جمع می‌کند و سپس تقسیم را انجام می‌دهد. در این صورت اگر کلمه‌ای از قبل در train وجود نداشته باشد, احتمال آن صفر نمی‌شود. این روش باعث بالا رفتن ارزیابی می‌شود."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"inputs/comment_train.csv\"\n",
    "TEST_PATH = \"inputs/comment_test.csv\"\n",
    "TITLE = \"title\"\n",
    "COMMENT = \"comment\"\n",
    "RECOMMEND = \"recommend\"\n",
    "RECOMMENDED = \"recommended\"\n",
    "NOTRECOMMENDED = \"not_recommended\"\n",
    "punctuations = ['٫', '.', '؟', '!', ';', '-', '_', '(', ')', '{', '}', '[', ']']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Problem():\n",
    "    def __init__(self, trainName, testName):\n",
    "        self.train = pd.read_csv(trainName)\n",
    "        self.test = pd.read_csv(testName)\n",
    "\n",
    "        \n",
    "        self.trainRowsNumber = self.train.shape[0]\n",
    "        self.testRowsNumber = self.test.shape[0]\n",
    "        \n",
    "        self.trainRowTitleWords = []\n",
    "        self.trainRowCommentWords = []\n",
    "        \n",
    "        self.testRowTitleWords = []\n",
    "        self.testRowCommentWords = []\n",
    "        \n",
    "        self.recommendedTitleWords = defaultdict(int)\n",
    "        self.recommendedCommentWords = defaultdict(int)\n",
    "        \n",
    "        self.notrecommendedTitleWords = defaultdict(int)\n",
    "        self.notrecommendedCommentWords = defaultdict(int)\n",
    "        \n",
    "        self.recommended = np.log((self.train[RECOMMEND] == RECOMMENDED).sum() / self.trainRowsNumber)\n",
    "        self.notrecommended = np.log((self.train[RECOMMEND] == NOTRECOMMENDED).sum() / self.trainRowsNumber)\n",
    "        \n",
    "        self.recommendCol = []\n",
    "        self.wrongDetectedComments = []\n",
    "        \n",
    "        self.accuracy = 0\n",
    "        self.precision = 0\n",
    "        self.recall = 0\n",
    "        self.f1 = 0\n",
    "    \n",
    "    def preProcess(self, _type):        \n",
    "        if _type == \"stemming\":\n",
    "            self.stemming()\n",
    "        elif _type == \"lemmatization\":\n",
    "            self.lemmatization()\n",
    "    \n",
    "    def normalize(self):\n",
    "        normalizer = Normalizer()\n",
    "        trainTitle = list(self.train[TITLE])\n",
    "        trainComment = list(self.train[COMMENT])\n",
    "                \n",
    "        for i in range(self.trainRowsNumber):\n",
    "            trainTitle[i] = normalizer.normalize(trainTitle[i])\n",
    "            trainComment[i] = normalizer.normalize(trainComment[i])\n",
    "            \n",
    "#             for token in stopwords_list():\n",
    "#                 trainTitle[i] = trainTitle[i].replace(token, \" \")\n",
    "#                 trainComment[i] = trainComment[i].replace(token, \" \")\n",
    "                \n",
    "            for punc in punctuations:\n",
    "                trainTitle[i] = trainTitle[i].replace(punc, \"\")\n",
    "                trainComment[i] = trainComment[i].replace(punc, \"\")\n",
    "                \n",
    "        self.train[TITLE] = trainTitle\n",
    "        self.train[COMMENT] = trainComment\n",
    "\n",
    "        testTitle = list(self.test[TITLE])\n",
    "        testComment = list(self.test[COMMENT])\n",
    "\n",
    "        for i in range(self.testRowsNumber):\n",
    "            testTitle[i] = normalizer.normalize(testTitle[i])\n",
    "            testComment[i] = normalizer.normalize(testComment[i])\n",
    "\n",
    "#             for token in stopwords_list():\n",
    "#                 testTitle[i] = testTitle[i].replace(token, \" \")\n",
    "#                 testComment[i] = testComment[i].replace(token, \" \")\n",
    "\n",
    "            for punc in punctuations:\n",
    "                testTitle[i] = testTitle[i].replace(punc, \"\")\n",
    "                testComment[i] = testComment[i].replace(punc, \"\")\n",
    "            \n",
    "        self.test[TITLE] = testTitle\n",
    "        self.test[COMMENT] = testComment\n",
    "        \n",
    "        \n",
    "    def wordTokenize(self):\n",
    "        trainTitle = list(self.train[TITLE])\n",
    "        trainComment = list(self.train[COMMENT])\n",
    "        for i in range(self.trainRowsNumber):\n",
    "            self.trainRowTitleWords.append(word_tokenize(trainTitle[i]))\n",
    "            self.trainRowCommentWords.append(word_tokenize(trainComment[i]))\n",
    "            \n",
    "        testTitle = list(self.test[TITLE])\n",
    "        testComment = list(self.test[COMMENT])\n",
    "        for i in range(self.testRowsNumber):\n",
    "            self.testRowTitleWords.append(word_tokenize(testTitle[i]))\n",
    "            self.testRowCommentWords.append(word_tokenize(testComment[i]))\n",
    "            \n",
    "\n",
    "            \n",
    "    def stemming(self):\n",
    "        stemmer = Stemmer()\n",
    "        \n",
    "        for i in range(self.trainRowsNumber):\n",
    "            size = len(self.trainRowTitleWords[i])\n",
    "            for j in range(size):\n",
    "                self.trainRowTitleWords[i][j] = stemmer.stem(self.trainRowTitleWords[i][j])\n",
    "                \n",
    "            size = len(self.trainRowCommentWords[i])\n",
    "            for j in range(size):\n",
    "                self.trainRowCommentWords[i][j] = stemmer.stem(self.trainRowCommentWords[i][j])\n",
    "        \n",
    "        for i in range(self.testRowsNumber):\n",
    "            size = len(self.testRowTitleWords[i])\n",
    "            for j in range(size):\n",
    "                self.testRowTitleWords[i][j] = stemmer.stem(self.testRowTitleWords[i][j])\n",
    "                \n",
    "            size = len(self.testRowCommentWords[i])\n",
    "            for j in range(size):\n",
    "                self.testRowCommentWords[i][j] = stemmer.stem(self.testRowCommentWords[i][j])\n",
    "                                \n",
    "    def lemmatization(self):\n",
    "        lemmatizer = Lemmatizer()\n",
    "        \n",
    "        for i in range(self.trainRowsNumber):\n",
    "            size = len(self.trainRowTitleWords[i])\n",
    "            for j in range(size):\n",
    "                self.trainRowTitleWords[i][j] = lemmatizer.lemmatize(self.trainRowTitleWords[i][j])\n",
    "                \n",
    "            size = len(self.trainRowCommentWords[i])\n",
    "            for j in range(size):\n",
    "                self.trainRowCommentWords[i][j] = lemmatizer.lemmatize(self.trainRowCommentWords[i][j])\n",
    "        \n",
    "        for i in range(self.testRowsNumber):\n",
    "            size = len(self.testRowTitleWords[i])\n",
    "            for j in range(size):\n",
    "                self.testRowTitleWords[i][j] = lemmatizer.lemmatize(self.testRowTitleWords[i][j])\n",
    "                \n",
    "            size = len(self.testRowCommentWords[i])\n",
    "            for j in range(size):\n",
    "                self.testRowCommentWords[i][j] = lemmatizer.lemmatize(self.testRowCommentWords[i][j])\n",
    "\n",
    "                \n",
    "    def training(self):\n",
    "        recommend = list(self.train[RECOMMEND])\n",
    "        for i in range(self.trainRowsNumber):\n",
    "            for j in range(len(self.trainRowTitleWords[i])):\n",
    "                word = self.trainRowTitleWords[i][j]\n",
    "                if recommend[i] == RECOMMENDED:\n",
    "                    self.recommendedTitleWords[word] += 1\n",
    "                else:\n",
    "                    self.notrecommendedTitleWords[word] += 1\n",
    "\n",
    "                    \n",
    "            for j in range(len(self.trainRowCommentWords[i])):\n",
    "                word = self.trainRowCommentWords[i][j]\n",
    "                if recommend[i] == RECOMMENDED:\n",
    "                    self.recommendedCommentWords[word] += 1\n",
    "                else:\n",
    "                    self.notrecommendedCommentWords[word] += 1\n",
    "                    \n",
    "\n",
    "    def process(self):\n",
    "        totalRecommendedTitleWords = sum(self.recommendedTitleWords.values(), 0.0)\n",
    "        totalRecommendedCommentWords = sum(self.recommendedCommentWords.values(), 0.0)\n",
    "        totalNotRecommededTitleWords = sum(self.notrecommendedTitleWords.values(), 0.0)\n",
    "        totalNotRecommendedCommentWords = sum(self.notrecommendedCommentWords.values(), 0.0)\n",
    "        \n",
    "        for i in range(self.testRowsNumber):\n",
    "            pRecommended = self.recommended\n",
    "            pNotRecommended = self.notrecommended\n",
    "            for j in range(len(self.testRowTitleWords[i])):\n",
    "                word = self.testRowTitleWords[i][j]\n",
    "                pRecommended += np.log(self.recommendedTitleWords[word] / totalRecommendedTitleWords)\n",
    "                pNotRecommended += np.log(self.notrecommendedTitleWords[word] / totalNotRecommededTitleWords)\n",
    "            \n",
    "            for j in range(len(self.testRowCommentWords[i])):\n",
    "                word = self.testRowCommentWords[i][j]\n",
    "                pRecommended += np.log(self.recommendedCommentWords[word] / totalRecommendedCommentWords)\n",
    "                pNotRecommended += np.log(self.notrecommendedCommentWords[word] / totalNotRecommendedCommentWords)\n",
    "                \n",
    "            if pRecommended > pNotRecommended:\n",
    "                self.recommendCol.append(RECOMMENDED)\n",
    "            else:\n",
    "                self.recommendCol.append(NOTRECOMMENDED)\n",
    "            \n",
    "    def processAdditive(self):\n",
    "        totalRecommendedTitleWords = sum(self.recommendedTitleWords.values(), 0.0)\n",
    "        totalRecommendedCommentWords = sum(self.recommendedCommentWords.values(), 0.0)\n",
    "        totalNotRecommededTitleWords = sum(self.notrecommendedTitleWords.values(), 0.0)\n",
    "        totalNotRecommendedCommentWords = sum(self.notrecommendedCommentWords.values(), 0.0)\n",
    "        \n",
    "        for i in range(self.testRowsNumber):\n",
    "            pRecommended = self.recommended\n",
    "            pNotRecommended = self.notrecommended\n",
    "            size = len(self.testRowTitleWords[i])\n",
    "            for j in range(size):\n",
    "                word = self.testRowTitleWords[i][j]    \n",
    "\n",
    "                pRecommended += np.log((self.recommendedTitleWords[word] + 1) / (totalRecommendedTitleWords + size))\n",
    "                pNotRecommended += np.log((self.notrecommendedTitleWords[word] + 1) / (totalNotRecommededTitleWords + size))\n",
    "            \n",
    "            size = len(self.testRowCommentWords[i])\n",
    "            for j in range(size):\n",
    "                word = self.testRowCommentWords[i][j]\n",
    "                pRecommended += np.log((self.recommendedCommentWords[word] + 1) / (totalRecommendedCommentWords + size))\n",
    "                pNotRecommended += np.log((self.notrecommendedCommentWords[word] + 1) / (totalNotRecommendedCommentWords + size))\n",
    "                \n",
    "            if pRecommended > pNotRecommended:\n",
    "                self.recommendCol.append(RECOMMENDED)\n",
    "            else:\n",
    "                self.recommendCol.append(NOTRECOMMENDED)\n",
    "                \n",
    "    def evaluate(self):\n",
    "        recommend = list(self.test[RECOMMEND])\n",
    "        title = list(self.test[TITLE])\n",
    "        comment = list(self.test[COMMENT])\n",
    "        correct = 0\n",
    "        correctRecommended = 0\n",
    "        totalRecommended = 0\n",
    "        detectedRecommended = 0\n",
    "        for i in range(self.testRowsNumber):\n",
    "            if self.recommendCol[i] == recommend[i]:\n",
    "                correct += 1\n",
    "                if recommend[i] == RECOMMENDED:\n",
    "                    correctRecommended += 1\n",
    "            else:\n",
    "                self.wrongDetectedComments.append((title[i], comment[i], recommend[i]))\n",
    "                    \n",
    "            if recommend[i] == RECOMMENDED:\n",
    "                totalRecommended += 1\n",
    "            \n",
    "            if self.recommendCol[i] == RECOMMENDED:\n",
    "                detectedRecommended += 1\n",
    "\n",
    "        self.accuracy = correct / self.testRowsNumber * 100.0\n",
    "        self.precision = correctRecommended / detectedRecommended * 100.0\n",
    "        self.recall = correctRecommended / totalRecommended * 100.0\n",
    "        self.f1 = 2 * (self.precision * self.recall) / (self.precision + self.recall)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## سوال ۵:\n",
    "### مقدار precision دقت تشخیص نظرات recommended را نشان می‌شود. مثال زیر را در نظر بگیرید:\n",
    "#### اگر تعداد کل نظرات recommended صدتا باشد و ما ۴ نظر را recommended تشخیص بدهیم که همه آنها هم درست باشند, دقت ما ۱۰۰ درصد است ولی تنها ۴ درصد نظرات را درست تشخیص داده ایم.\n",
    "### مقدار recall  نسبت نظرات recommeded ای که ما تشخیص داده‌ایم به کل نظرات recommended موجود را نشان می‌دهد. به مثال زیر توجه کنید:\n",
    "#### اگر ما ۱۰۰ نظر recommended و ۱۰۰ نظر not recommended داشته باشیم ولی تمام نظرات را recommended تشخیص دهیم, مقدار recall برابر ۱۰۰ درصد خواهد بود ولی دقت ما ۵۰ درصد می‌شود.\n",
    "### بنابراین این دو مقدار با هم معنا پیدا می‌کنند."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## سوال ۶:\n",
    "### میانگین گیری معکوس: اگر یکی از متغیرهای precision و recall مقدار کمی داشته باشد و دیگری مقدار زیادی داشته باشد, میانگین عادی این دو عدد نسبتا بزرگی می‌شود که اشتباه است. یا میانگین گیری معکوس این دو متغیر, میانگین به عددی که کوچکتر است نزدیکتر می‌شود و در نتیجه ارزیابی بهتری را نشان می‌دهد"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## سوال ۷:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  94.0\n",
      "Precision:  93.56435643564357\n",
      "Recall:  94.5\n",
      "F1: 94.02985074626868\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "problem = Problem(TRAIN_PATH, TEST_PATH)\n",
    "problem.normalize()\n",
    "problem.wordTokenize()\n",
    "# problem.preProcess(\"lemmatization\")\n",
    "problem.training()\n",
    "problem.processAdditive()\n",
    "problem.evaluate()\n",
    "\n",
    "wrongDetected = problem.wrongDetectedComments[:5]\n",
    "\n",
    "print(\"Accuracy: \", problem.accuracy)\n",
    "print(\"Precision: \", problem.precision)\n",
    "print(\"Recall: \", problem.recall)\n",
    "print(\"F1:\", problem.f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.5\n",
      "Precision:  93.5\n",
      "Recall:  93.5\n",
      "F1: 93.5\n"
     ]
    }
   ],
   "source": [
    "#A Lemmatization\n",
    "problem = Problem(TRAIN_PATH, TEST_PATH)\n",
    "problem.normalize()\n",
    "problem.wordTokenize()\n",
    "problem.preProcess(\"lemmatization\")\n",
    "problem.training()\n",
    "problem.processAdditive()\n",
    "problem.evaluate()\n",
    "\n",
    "print(\"Accuracy: \", problem.accuracy)\n",
    "print(\"Precision: \", problem.precision)\n",
    "print(\"Recall: \", problem.recall)\n",
    "print(\"F1:\", problem.f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.375\n",
      "Precision:  93.26683291770573\n",
      "Recall:  93.5\n",
      "F1: 93.3832709113608\n"
     ]
    }
   ],
   "source": [
    "#A Stemming\n",
    "problem = Problem(TRAIN_PATH, TEST_PATH)\n",
    "problem.normalize()\n",
    "problem.wordTokenize()\n",
    "problem.preProcess(\"stemming\")\n",
    "problem.training()\n",
    "problem.processAdditive()\n",
    "problem.evaluate()\n",
    "\n",
    "print(\"Accuracy: \", problem.accuracy)\n",
    "print(\"Precision: \", problem.precision)\n",
    "print(\"Recall: \", problem.recall)\n",
    "print(\"F1:\", problem.f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  94.125\n",
      "Precision:  93.58024691358024\n",
      "Recall:  94.75\n",
      "F1: 94.1614906832298\n"
     ]
    }
   ],
   "source": [
    "#B\n",
    "problem = Problem(TRAIN_PATH, TEST_PATH)\n",
    "# problem.normalize()\n",
    "problem.wordTokenize()\n",
    "# problem.preProcess(\"lemmatization\")\n",
    "problem.training()\n",
    "problem.processAdditive()\n",
    "problem.evaluate()\n",
    "\n",
    "print(\"Accuracy: \", problem.accuracy)\n",
    "print(\"Precision: \", problem.precision)\n",
    "print(\"Recall: \", problem.recall)\n",
    "print(\"F1:\", problem.f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  87.375\n",
      "Precision:  95.16616314199395\n",
      "Recall:  78.75\n",
      "F1: 86.18331053351574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-207-5d6720a8fa2d>:167: RuntimeWarning: divide by zero encountered in log\n",
      "  pNotRecommended += np.log(self.notrecommendedTitleWords[word] / totalNotRecommededTitleWords)\n",
      "<ipython-input-207-5d6720a8fa2d>:166: RuntimeWarning: divide by zero encountered in log\n",
      "  pRecommended += np.log(self.recommendedTitleWords[word] / totalRecommendedTitleWords)\n",
      "<ipython-input-207-5d6720a8fa2d>:171: RuntimeWarning: divide by zero encountered in log\n",
      "  pRecommended += np.log(self.recommendedCommentWords[word] / totalRecommendedCommentWords)\n",
      "<ipython-input-207-5d6720a8fa2d>:172: RuntimeWarning: divide by zero encountered in log\n",
      "  pNotRecommended += np.log(self.notrecommendedCommentWords[word] / totalNotRecommendedCommentWords)\n"
     ]
    }
   ],
   "source": [
    "#C\n",
    "problem = Problem(TRAIN_PATH, TEST_PATH)\n",
    "problem.normalize()\n",
    "problem.wordTokenize()\n",
    "problem.preProcess(\"lemmatization\")\n",
    "problem.training()\n",
    "problem.process()\n",
    "problem.evaluate()\n",
    "\n",
    "print(\"Accuracy: \", problem.accuracy)\n",
    "print(\"Precision: \", problem.precision)\n",
    "print(\"Recall: \", problem.recall)\n",
    "print(\"F1:\", problem.f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  87.125\n",
      "Precision:  96.26168224299066\n",
      "Recall:  77.25\n",
      "F1: 85.71428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-207-5d6720a8fa2d>:167: RuntimeWarning: divide by zero encountered in log\n",
      "  pNotRecommended += np.log(self.notrecommendedTitleWords[word] / totalNotRecommededTitleWords)\n",
      "<ipython-input-207-5d6720a8fa2d>:166: RuntimeWarning: divide by zero encountered in log\n",
      "  pRecommended += np.log(self.recommendedTitleWords[word] / totalRecommendedTitleWords)\n",
      "<ipython-input-207-5d6720a8fa2d>:171: RuntimeWarning: divide by zero encountered in log\n",
      "  pRecommended += np.log(self.recommendedCommentWords[word] / totalRecommendedCommentWords)\n",
      "<ipython-input-207-5d6720a8fa2d>:172: RuntimeWarning: divide by zero encountered in log\n",
      "  pNotRecommended += np.log(self.notrecommendedCommentWords[word] / totalNotRecommendedCommentWords)\n"
     ]
    }
   ],
   "source": [
    "#D\n",
    "problem = Problem(TRAIN_PATH, TEST_PATH)\n",
    "# problem.normalize()\n",
    "problem.wordTokenize()\n",
    "# problem.preProcess(\"lemmatization\")\n",
    "problem.training()\n",
    "problem.process()\n",
    "problem.evaluate()\n",
    "\n",
    "print(\"Accuracy: \", problem.accuracy)\n",
    "print(\"Precision: \", problem.precision)\n",
    "print(\"Recall: \", problem.recall)\n",
    "print(\"F1:\", problem.f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## سوال ۸:\n",
    "### همانطور که پیشبینی میشد با روش additive smoothing به نتیجه بهتری رسیدیم. ولی پیش پردازش باعث کم شدن دقت و F1 می‌شود. دلیل آن این است که همانطور که در سوال اول گفته شد, روش lemmatization کنار مزایایی که دارد اشکالاتی نیز دارد. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('نقد پس از خرید', 'سلام، راحت شدم از کابل شارژ، توصیه میشود به شدت ارزان گوشی خود را به شارژ وایرلس مجهز کنید', 'recommended')\n",
      "\n",
      "('خیالم راحت شد', 'فندک قبلیم مدام فیوز میسوزوند و یک بار شارژر موبایل هم سوزوند ولی با این هیچ مشکلی بوجود نیومده تا الان کیفیتش خیلی خوبه و لامپ هم داره', 'recommended')\n",
      "\n",
      "('جنس و زیبایی', 'زیبا هستش از مدل\\u200cهای دیگه مثل پارس … بنظرم زیبا\\u200cتر با کیفیت\\u200cتر هستش', 'recommended')\n",
      "\n",
      "('بررسی فیلتر سرکان', 'من خودم جزو افرادی بودم که نزدیک سیزده ساله از انواع فیلتر سرکان اعم از روغن، هوا و اتاق استفاده میکردم ولی به تازگی متوجه و اطلاع یافتم که فیلتر گاج باکیفیت\\u200cتر از فیلتر سرکان می\\u200cباشد و هم چنین قیمت بمراتب مناسب تربت نسبت به سرکان دارد و طرف فروشنده که داشت روغن فیلترش را به من می\\u200cفروخت واقعا بهم اثبات کرد که گاج باکیفیت\\u200cتر از سرکان می\\u200cباشد', 'not_recommended')\n",
      "\n",
      "('کل صفحه رو نمیپوشونه', 'جنسش و چسبندگیش عالیه اما کل صفحه رو نمیپوشونه متاسفانه با این دید اگر بخرین، ناراضی نمیشید از خریدتون مثل من', 'not_recommended')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(wrongDetected[i], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## سوال ۹:\n",
    "### تعریف کردن از محصولات دیگر را نمی‌تواند تشخیص بدهد و به اشتباه فکر می‌کند تعاریف برای این محصول است و همینطور برعکس. اشکالات بقیه محصولات را گفتن باعث اشتباه در تشخیص می‌شود.\n",
    "### اگر بتوان ریشه افعال را بدون از دست دادن منفی یا مثبت بودن آن نگه داشت می‌توان نتیجه را بهبود داد."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
